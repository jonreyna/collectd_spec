% !TEX root = ../main.tex
%
\chapter{Introduction}\label{sec:intro}

\cleanchapterquote{The art of programming is the art of organizing complexity,
of mastering multitude and avoiding its bastard chaos as effectively as 
possible}{E. W. Dijkstra}{(Computer Science Pioneer)}

Modern network automation is generally achieved using a group of small to medium
sized software programs developed in isolation of each other. Dispite their isolation,
most programs have the same general flow of logic. Almost every network automation
tool can be described on a high level with these simple steps.
\begin{figure}[h]
\begin{minipage}{.5\textwidth}
	\begin{enumerate}
		\item Get the data from devices that contains the answer to a 
			question of interest. The answer, at least in part, 
			satisfies a business requirement.
		\item Take steps to convert the raw data into information. 
			Common steps used to convert the data include the 
			following:
			\begin{itemize}[noitemsep]
				\item Isolate specific data points of interest.
				\item Convert data points to strong types for 
					safe comparison operations.
				\item Compare parsed data with other gathered 
					data or constant values to answer a 
					question of interest.
			\end{itemize}
		\item Perform some action based on the answer to the question 
			of interest. Common actions include the following:
			\begin{itemize}[noitemsep]
				\item Update a webpage.
				\item Send an email.
				\item Configure a device.
			\end{itemize}
	\end{enumerate}
\end{minipage}
\begin{minipage}{0.5\textwidth}
	\includegraphics[width=\textwidth]{gfx/general_tool_logic_flow}
\end{minipage}
\end{figure}

The general logic flow of homogenous applications is helpful when identifying 
where that logic can be consolidated. The act of making a general logic flow 
forces the author to generalize about application logic. The extent that logic 
can be generalized is equivalent to the extent that it can be consolidated.
Data collection is (arguably) the most generalized logic in the above flow. It
follows, therefore, that consolidating its code would be the most beneficial.
%Choosing the most generalized logic will help Maximizing logic consolidation will help speed up development
%It therefore follows that the more logic that can be consolidated the most will have
%the 
%The inventory tool has logic that is easy to understand, and can serve as a 
%concrete example of the logic flow above. During step 1, the inventory tool
%uses the NETCONF equivalent of the \texttt{show chassis hardware detail} command to retrieve
%the required inventory data. Step 2 is achieved by storing the inventory
%data in a relational database. Finally, in step 3, the tool answers user queries 
%A concrete example can serve to illustrate the above logic flow.
\section{Benefits of Consolidated Data Collection}
\label{sec:intro:address}

Data collection logic consolidation has many benefits that improve application 
quality and delivery time. Some of them derive from a drastic simplification in
the application logic. An application's code will be reduced almost entirely to
business logic, which makes it easier to reason about. Other benefits are come
from the shared collection data. In the best case, the shared data will save a 
developer from writing code to collect it. In the worst case, new code must be
added to the collection process. However, that collection code only needs to be
written once for it to be easily available to all applications.

Moreover collected data
is shared between all applications. In the best case, an application developer won't even need to
write code to collect data., because it will have been written already

Consolidating data collection code makes it is easier to test. Unit tests
(code that is written to test a unit of logic) only need to be written for the 
NETCONF, SSH, and SQL code once. This will drastically improve testing quality,
which ultimately improves application quality.

The majority of logic consolidation benefits come from time savings.

Most benefits of logic consolidation benefits come in the form of time savings.
The majority of the benefits realized from logic consolidation 

\section{Motivation and Problem Statement}
\label{sec:intro:motivation}

blah blah blah
In light of the first two steps in the general flow of logic, one only needs a
trivial example to show how this and that each 
project is developed in isolation, it's easy to see how 
In 
There are at least two approaches to remedy these inefficiencies. 

Modern network automation is typically implemented as a small or medium sized
software program that answers questions about, or changes the state of, a 
network. Conventional projects 
A network automation project, loosely defined, is a small to medium sized 
software program that answers questions about, or change the state of, 
a network. Conventional practices isolate their development lifecycle
from other projects. In part, the isolation is desireable for maximum flexibility in the 
implementation. Developers aren't constrained by design limitations of previous
projects, and can make the best descisions for the project at hand. That 
flexibility, however, comes at a cost: effort duplication. 

Effort duplication is most noticible in the data collection code required 
by most network automation tools. Since projects are deveoped in isolation, 
sharing data with other applications is an afterthought. Code, data,
compute, maintenance, and programming effort is often duplicated. There are at least
two approaches to remedy these inefficiencies. 

The first enforces strict rules on the development lifecycle. New rules
would require every project to consider every other project in its design phase. 
The flexibility of an isolated project is sacrified for global design consistency.
At best, project design complexity would increase on a linear scale, and 
exponentially in the worst case.

A more practical approach would adapt an architectural pattern to the to the 
existing development lifecycle. The pattern would eliminate duplication
efforts by consolidating homogenous logic to make the results available to
client applications. Collectd is an instance of this pattern. 

Collectd is responsible for collecting, storing, relating, and providing data retrieved 
from network devices to client applications. It consolidates all data 
collection logic, alleviating client applications of that responsibility.
When a tool needs data, it simply requests it from collectd. 

Collectd has numerous advantages over an isolated project's data collection approach. 
Client applications no longer need collection code, making them easier to reason
about, and faster to enhance, maintain, test, and document. Collection code will
be more stable and robust, because writing collection code will be concentrated,
instead of duplicated. Without the inefficiencies of a strict lifecycle approach, 
applications will maintain maximum flexibility to satisfy business requirements.
There will be fine grained control over SSH sessions to devices. Collectd will
have a global perspective of collected data from all applications, and the 
relationships between that data, which offers unique potential to answer 
questions that have not yet been considered, or were thought impractical to 
answer.
\Blindtext[3][1]~\cite{Jurgens:2000,Jurgens:1995,Miede:2011,Kohm:2011,Apple:keynote:2010,Apple:numbers:2010,Apple:pages:2010}

\section{Results}
\label{sec:intro:results}

\Blindtext[1][2]

\subsection{Some References}
\label{sec:intro:results:refs}
\cite{WEB:GNU:GPL:2010,WEB:Miede:2011}

\section{Thesis Structure}
\label{sec:intro:structure}

\textbf{Chapter~\ref{sec:related}} \\[0.2em]
\blindtext{}

\textbf{Chapter~\ref{sec:system}} \\[0.2em]
\blindtext{}

\textbf{Chapter~\ref{sec:concepts}} \\[0.2em]
\blindtext{}

\textbf{Chapter~\ref{sec:concepts}} \\[0.2em]
\blindtext{}

\textbf{Chapter~\ref{sec:conclusion}} \\[0.2em]
\blindtext{}
